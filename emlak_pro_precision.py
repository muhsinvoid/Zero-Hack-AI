import pandas as pd
import streamlit as st
import os
import re
import numpy as np
import folium
from streamlit_folium import st_folium
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from datetime import datetime

# --- SABÄ°T TANIMLAR ---
CURRENT_YEAR = datetime.now().year

# --- DÄ°L SÃ–ZLÃœÄÃœ (TAM VE EKSÄ°KSÄ°Z) ---
TRANS = {
    "TR": {
        "title": "ZERO HACK: EMLAK PRO PLATINUM (SAF VERÄ°)", "upload_label": "Veri Seti (CSV)", "upload_success": "Ä°lan YÃ¼klendi",
        "btn_analyze": "ANALÄ°ZÄ° BAÅLAT", "metric_acc": "AI DoÄŸruluk Skoru (RÂ²)", "metric_total": "Toplam Ä°lan",
        "metric_opp": "SÃ¼per FÄ±rsat", "metric_err": "Temizlenen KayÄ±t", 
        "tab_detail": "ğŸ“Š DETAYLI ANALÄ°Z", "tab_opp": "ğŸ’ FIRSATLAR", "tab_comp": "ğŸ¯ OPTIMAL SEÃ‡Ä°M",
        "tab_map": "ğŸ—ºï¸ HARÄ°TA", "tab_ai": "ğŸ¤– AI ASÄ°STAN", "col_price": "Fiyat", "col_ai": "AI DeÄŸer",
        "col_ref": "2020 Ref.", "col_diff": "Fark", "col_rooms": "Oda SayÄ±sÄ±", "col_age": "Bina YaÅŸÄ±",
        "col_bath": "Banyo SayÄ±sÄ±", "col_heating": "IsÄ±tma",
        "status_err_low": "â›” HATALI (SÄ°LÄ°NDÄ°)", "status_opp": "ğŸ’ SÃœPER FIRSAT", "status_ok": "âœ… PÄ°YASA UYGUN",
        "status_err_high": "âŒ PAHALI (SÄ°LÄ°NDÄ°)", "chat_placeholder": "Veri setine bir soru sor...",
        "ai_unknown": "AnlayamadÄ±m. LÃ¼tfen kÄ±lavuza gÃ¶z atÄ±n.", "loading": "Model eÄŸitiliyor ve veriler iÅŸleniyor...",
        "comp_rooms": "Oda SayÄ±sÄ±na GÃ¶re", "comp_price": "Fiyata GÃ¶re", "comp_m2": "mÂ²'ye GÃ¶re",
        "comp_select_title": "KarÅŸÄ±laÅŸtÄ±rma Kriterini SeÃ§in", "comp_list_title": "En MantÄ±klÄ± {kriter} Ä°lanlar (ROI'ye GÃ¶re)",
        "map_title": "BÃ¶lgesel YoÄŸunluk ve Fiyat HaritasÄ±", "err_no_file": "LÃ¼tfen CSV dosyasÄ±nÄ± yÃ¼kleyin.",
        "box_opp_title": "ğŸ’ YATIRIMLIK FIRSATLAR", "box_err_title": "ğŸš« HATA KAYITLARI (GÄ°ZLENDÄ°)",
        "chat_intro": "ğŸ’¡ **Ä°PUCU:** Ben sadece yÃ¼klenen dosya hakkÄ±ndaki sorularÄ± yanÄ±tlarÄ±m.",
        "cleaning_report": "âœ… **VERÄ° TEMÄ°ZLÄ°ÄÄ°:** BaÅŸlangÄ±Ã§: {initial}. AtÄ±lan (Teknik + MantÄ±ksÄ±z + AI HatalÄ±): {removed}. **Analiz {final} ilanla baÅŸladÄ±.**",
        "val_none": "Yok",
        "ai_guide_title": "AI ASÄ°STAN Soru KÄ±lavuzu",
        "ai_guide_content": """
        Bu veri setine ÅŸunlarÄ± sorabilirsiniz:
        * **Genel Metrikler:** "Toplam kaÃ§ ilan var?", "En pahalÄ± ev nerede?", "En ucuz daire hangisi?"
        * **BÃ¶lge BazlÄ±:** "KadÄ±kÃ¶y'deki ortalama fiyat ne kadar?", "Esenyurt'ta kaÃ§ ilan var?"
        * **Birim Fiyat:** "Ortalama mÂ² fiyatÄ± nedir?", "Ortalama Bina yaÅŸÄ± nedir?"
        """
    },
    "EN": {
        "title": "ZERO HACK: ESTATE PRO PLATINUM (PURE DATA)", "upload_label": "Dataset (CSV)", "upload_success": "Loaded",
        "btn_analyze": "START ANALYSIS", "metric_acc": "AI Accuracy Score (RÂ²)", "metric_total": "Total Listings",
        "metric_opp": "Super Opportunity", "metric_err": "Records Cleared", 
        "tab_detail": "ğŸ“Š DETAILED ANALYSIS", "tab_opp": "ğŸ’ OPPORTUNITIES", "tab_comp": "ğŸ¯ OPTIMAL SELECTION", "tab_map": "ğŸ—ºï¸ MAP", "tab_ai": "ğŸ¤– AI ASSISTANT",
        "col_price": "Price", "col_ai": "AI Value", "col_ref": "2020 Ref.", "col_diff": "Diff",
        "col_rooms": "Rooms", "col_age": "Age", "col_bath": "Bathrooms", "col_heating": "Heating",
        "status_err_low": "â›” ERROR (DELETED)", "status_opp": "ğŸ’ SUPER OPPORTUNITY", "status_ok": "âœ… MARKET PRICE",
        "status_err_high": "âŒ OVERPRICED (DELETED)", "chat_placeholder": "Ask a question to the dataset...",
        "ai_unknown": "I didn't understand. Please check the guide.", "loading": "Training model...",
        "comp_rooms": "By Number of Rooms", "comp_price": "By Price", "comp_m2": "By mÂ²",
        "comp_select_title": "Select Comparison Criteria",
        "comp_list_title": "Most Optimal {kriter} Listings (By ROI)", "map_title": "Regional Density and Price Map", 
        "err_no_file": "Please upload a CSV file.", "box_opp_title": "ğŸ’ INVESTMENT OPPORTUNITIES (CLEAN DATA)", "box_err_title": "ğŸš« ERROR RECORDS (HIDDEN)",
        "chat_intro": "ğŸ’¡ **TIP:** I only answer questions about the uploaded file.",
        "cleaning_report": "âœ… **CLEANING REPORT:** Initial: {initial}. Removed (Impossible/Technical Error): {removed}. **Analysis started with {final} listings.**",
        "val_none": "None",
        "ai_guide_title": "AI ASSISTANT Question Guide",
        "ai_guide_content": """
        You can ask the dataset:
        * **General Metrics:** "How many listings are there?", "Where is the most expensive house?", "What is the cheapest flat?"
        * **District Based:** "What is the average price in Kadikoy?", "How many listings are in Esenyurt?"
        * **Price Per MÂ²:** "What is the average price per mÂ²?", "What is the average building age?"
        """
    },
    "RU": {
        "title": "ZERO HACK: ESTATE PRO PLATINUM (Ğ§Ğ˜Ğ¡Ğ¢Ğ«Ğ• Ğ”ĞĞĞĞ«Ğ•)", "upload_label": "CSV Ğ¤Ğ°Ğ¹Ğ»", "upload_success": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾",
        "btn_analyze": "ĞĞĞ§ĞĞ¢Ğ¬ ĞĞĞĞ›Ğ˜Ğ—", "metric_acc": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ (RÂ²)", "metric_total": "Ğ’ÑĞµĞ³Ğ¾", "metric_opp": "Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ", 
        "metric_err": "Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ Ğ—Ğ°Ğ¿Ğ¸ÑĞµĞ¹", "tab_detail": "ğŸ“Š ĞĞĞĞ›Ğ˜Ğ—", "tab_opp": "ğŸ’ Ğ’Ğ«Ğ“ĞĞ”ĞĞ", "tab_comp": "ğŸ¯ Ğ’Ğ«Ğ‘ĞĞ ", "tab_map": "ğŸ—ºï¸ ĞšĞĞ Ğ¢Ğ", 
        "tab_ai": "ğŸ¤– Ğ˜Ğ˜", "col_price": "Ğ¦ĞµĞ½Ğ°", "col_ai": "Ğ˜Ğ˜ Ğ¦ĞµĞ½Ğ°", "col_ref": "2020 Ğ¡Ğ¿Ñ€.", "col_diff": "Ğ Ğ°Ğ·Ğ½.", "col_rooms": "ĞšĞ¾Ğ¼Ğ½Ğ°Ñ‚Ñ‹", 
        "col_age": "Ğ’Ğ¾Ğ·Ñ€Ğ°ÑÑ‚", "col_bath": "Ğ’Ğ°Ğ½Ğ½Ñ‹Ğµ", "col_heating": "ĞÑ‚Ğ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ", "status_err_low": "â›” ĞĞ¨Ğ˜Ğ‘ĞšĞ (Ğ£Ğ”ĞĞ›Ğ•ĞĞ)", "status_opp": "ğŸ’ Ğ’Ğ«Ğ“ĞĞ”ĞĞ", 
        "status_ok": "âœ… ĞĞĞ ĞœĞ", "status_err_high": "âŒ Ğ”ĞĞ ĞĞ“Ğ (Ğ£Ğ”ĞĞ›Ğ•ĞĞ)", "chat_placeholder": "Ğ¡Ğ¿Ñ€Ğ¾ÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ...", "ai_unknown": "Ğ¯ Ğ½Ğµ Ğ¿Ğ¾Ğ½ÑĞ». ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ³Ğ°Ğ¹Ğ´.",
        "loading": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...", "comp_rooms": "ĞŸĞ¾ ĞºĞ¾Ğ»-Ğ²Ñƒ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚", "comp_price": "ĞŸĞ¾ Ñ†ĞµĞ½Ğµ", "comp_m2": "ĞŸĞ¾ Ğ¼Â²",
        "comp_select_title": "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸", "comp_list_title": "Ğ¡Ğ°Ğ¼Ñ‹Ğµ Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ {kriter} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ", "map_title": "ĞšĞ°Ñ€Ñ‚Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ†ĞµĞ½",
        "err_no_file": "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» CSV.", "box_opp_title": "ğŸ’ Ğ˜ĞĞ’Ğ•Ğ¡Ğ¢Ğ˜Ğ¦Ğ˜Ğ˜ (Ğ§Ğ˜Ğ¡Ğ¢Ğ«Ğ• Ğ”ĞĞĞĞ«Ğ•)", "box_err_title": "ğŸš« Ğ—ĞĞŸĞ˜Ğ¡Ğ˜ ĞĞ¨Ğ˜Ğ‘ĞĞš (Ğ¡ĞšĞ Ğ«Ğ¢Ğ«)",
        "chat_intro": "ğŸ’¡ **Ğ¡ĞĞ’Ğ•Ğ¢:** Ğ¯ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñƒ.",
        "cleaning_report": "âœ… **ĞĞ¢Ğ§Ğ•Ğ¢:** ĞĞ°Ñ‡Ğ°Ğ»Ğ¾: {initial}. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ (Ğ˜Ğ·-Ğ·Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº): {removed}. **ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ñ‚ Ñ {final} Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹.**",
        "val_none": "ĞĞµÑ‚",
        "ai_guide_title": "Ğ“Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğº Ğ˜Ğ˜",
        "ai_guide_content": """
        Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ ÑĞ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ:
        * **ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸:** "Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹?", "Ğ“Ğ´Ğµ ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ğ´Ğ¾Ğ¼?", "ĞšĞ°ĞºĞ°Ñ ÑĞ°Ğ¼Ğ°Ñ Ğ´ĞµÑˆĞµĞ²Ğ°Ñ ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ°?"
        * **ĞŸĞ¾ Ğ Ğ°Ğ¹Ğ¾Ğ½Ğ°Ğ¼:** "ĞšĞ°ĞºĞ¾Ğ²Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ° Ğ² ĞšĞ°Ğ´Ñ‹ĞºĞµĞ¹?", "Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ­ÑĞµĞ½ÑŒÑÑ€Ñ‚?"
        * **Ğ¦ĞµĞ½Ğ° Ğ·Ğ° Ğ¼Â²:** "ĞšĞ°ĞºĞ¾Ğ²Ğ° ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ° Ğ·Ğ° Ğ¼Â²?", "ĞšĞ°ĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹?"
        """
    },
    "AR": {
        "title": "Ø²ÙŠØ±Ùˆ Ù‡Ø§Ùƒ: Ø¥Ù…Ù„Ø§Ùƒ Ø¨Ø±Ùˆ Ø¨Ù„Ø§ØªÙŠÙ†ÙŠÙˆÙ…", "upload_label": "Ù…Ù„Ù CSV", "upload_success": "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª",
        "btn_analyze": "Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„", "metric_acc": "Ø¯Ù‚Ø© AI", "metric_total": "Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ", "metric_opp": "ÙØ±ØµØ©", 
        "metric_err": "Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø·Ù‡Ø±Ø©", "tab_detail": "ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„", "tab_opp": "ğŸ’ Ø§Ù„ÙØ±Øµ", "tab_comp": "ğŸ¯ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ù…Ø«Ù„", "tab_map": "ğŸ—ºï¸ Ø§Ù„Ø®Ø±ÙŠØ·Ø©", 
        "tab_ai": "ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯", "col_price": "Ø§Ù„Ø³Ø¹Ø±", "col_ai": "Ù‚ÙŠÙ…Ø© AI", "col_ref": "Ù…Ø±Ø¬Ø¹ 2020", "col_diff": "Ø§Ù„ÙØ±Ù‚",
        "col_rooms": "Ø§Ù„ØºØ±Ù", "col_age": "Ø§Ù„Ø¹Ù…Ø±", "col_bath": "Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª", "col_heating": "Ø§Ù„ØªØ¯ÙØ¦Ø©",
        "status_err_low": "â›” Ø®Ø·Ø£ (Ù…Ø­Ø°ÙˆÙ)", "status_opp": "ğŸ’ ÙØ±ØµØ©", "status_ok": "âœ… Ø³Ø¹Ø± Ø§Ù„Ø³ÙˆÙ‚", "status_err_high": "âŒ Ù…Ø±ØªÙØ¹ (Ù…Ø­Ø°ÙˆÙ)", 
        "chat_placeholder": "Ø§Ø³Ø£Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...", "ai_unknown": "Ù„Ù… Ø£ÙÙ‡Ù…. ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¯Ù„ÙŠÙ„.", "loading": "Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...",
        "comp_rooms": "Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù", "comp_price": "Ø§Ù„Ø³Ø¹Ø±", "comp_m2": "Ø§Ù„Ù…Ø³Ø§Ø­Ø©", "comp_select_title": "Ø§Ø®ØªØ± Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©", 
        "comp_list_title": "Ø£ÙØ¶Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª {kriter}", "map_title": "Ø®Ø±ÙŠØ·Ø© Ø§Ù„ÙƒØ«Ø§ÙØ© ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±", "err_no_file": "ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV.", 
        "box_opp_title": "ğŸ’ ÙØ±Øµ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ© (Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø¸ÙŠÙØ©)", "box_err_title": "ğŸš« Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø£ (Ù…Ø®ÙÙŠØ©)", "chat_intro": "ğŸ’¡ **ØªÙ„Ù…ÙŠØ­:** Ø£Ù†Ø§ Ø£Ø¬ÙŠØ¨ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.",
        "cleaning_report": "âœ… **ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ:** Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: {initial}. ØªÙ… Ø¥Ø²Ø§Ù„Ø© (Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡): {removed}. **Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù€ {final} Ø¥Ø¹Ù„Ø§Ù†.**",
        "val_none": "Ù„Ø§ ÙŠÙˆØ¬Ø¯",
        "ai_guide_title": "Ø¯Ù„ÙŠÙ„ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¢Ù„ÙŠ",
        "ai_guide_content": """
        ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ³Ø£Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
        * **Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¹Ø§Ù…Ø©:** "ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§ØªØŸ", "Ø£ÙŠÙ† Ø£ØºÙ„Ù‰ Ù…Ù†Ø²Ù„ØŸ", "Ù…Ø§ Ù‡ÙŠ Ø£Ø±Ø®Øµ Ø´Ù‚Ø©ØŸ"
        * **Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©:** "Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø± ÙÙŠ ÙƒØ§Ø¯ÙŠÙƒÙˆÙŠØŸ", "ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠ Ø¥Ø³Ù†ÙŠÙˆØ±ØªØŸ"
        * **Ø§Ù„Ø³Ø¹Ø± Ù„Ù„Ù…ØªØ± Ø§Ù„Ù…Ø±Ø¨Ø¹:** "Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø³Ø¹Ø± Ø§Ù„Ù…ØªØ± Ø§Ù„Ù…Ø±Ø¨Ø¹ØŸ", "Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø¹Ù…Ø± Ø§Ù„Ù…Ø¨Ù†Ù‰ØŸ"
        """
    }
}

# --- REFERANS FÄ°YATLAR (2020) ve Ä°STANBUL KOORDÄ°NATLARI (KÄ±saltÄ±ldÄ±) ---
REF_PRICES_2020 = { "Esenyurt": 2150, "Sultanbeyli": 2250, "EyÃ¼psultan": 3400, "BeyoÄŸlu": 5500, "AtaÅŸehir": 5342, "BaÅŸakÅŸehir": 4544, "KÃ¼Ã§Ã¼kÃ§ekmece": 4076, "AvcÄ±lar": 2936, "BeylikdÃ¼zÃ¼": 2683, "KadÄ±kÃ¶y": 8067, "KaÄŸÄ±thane": 4172, "BÃ¼yÃ¼kÃ§ekmece": 3456, "BaÄŸcÄ±lar": 3255, "Ãœmraniye": 3767, "Silivri": 2582, "ÃœskÃ¼dar": 5010, "BeÅŸiktaÅŸ": 11788, "BakÄ±rkÃ¶y": 9207, "Sancaktepe": 2613, "Adalar": 6784, "Tuzla": 3262, "Kartal": 3749, "Pendik": 3098, "Ã‡ekmekÃ¶y": 3088, "ArnavutkÃ¶y": 2456, "Esenler": 3197, "Åile": 3836, "ÅiÅŸli": 5592, "Maltepe": 4205, "Sultangazi": 2661, "Zeytinburnu": 4546, "BayrampaÅŸa": 3701, "Ã‡atalca": 2802, "BahÃ§elievler": 3353, "GaziosmanpaÅŸa": 3106, "SarÄ±yer": 10589, "Fatih": 4202, "GÃ¼ngÃ¶ren": 3206 }
ISTANBUL_COORDS = {
    "Adalar": [40.8765, 29.1325], "ArnavutkÃ¶y": [41.1856, 28.7402], "AtaÅŸehir": [40.9932, 29.1132],
    "AvcÄ±lar": [40.9789, 28.7231], "BaÄŸcÄ±lar": [41.0343, 28.8576], "BahÃ§elievler": [41.0001, 28.8601],
    "BakÄ±rkÃ¶y": [40.9832, 28.8732], "BaÅŸakÅŸehir": [41.0976, 28.8071], "BayrampaÅŸa": [41.0354, 28.9123],
    "BeÅŸiktaÅŸ": [41.0428, 29.0076], "Beykoz": [41.1213, 29.0963], "BeylikdÃ¼zÃ¼": [40.9892, 28.6434],
    "BeyoÄŸlu": [41.0284, 28.9736], "BÃ¼yÃ¼kÃ§ekmece": [41.0321, 28.5872], "Ã‡atalca": [41.1432, 28.4593],
    "Ã‡ekmekÃ¶y": [41.0351, 29.1751], "Esenler": [41.0487, 28.8856], "Esenyurt": [41.0342, 28.6801],
    "EyÃ¼psultan": [41.0471, 28.9332], "Fatih": [41.0102, 28.9403], "GaziosmanpaÅŸa": [41.0581, 28.9124],
    "GÃ¼ngÃ¶ren": [41.0253, 28.8651], "KadÄ±kÃ¶y": [40.9901, 29.0254], "KaÄŸÄ±thane": [41.0812, 28.9753],
    "Kartal": [40.8901, 29.1901], "KÃ¼Ã§Ã¼kÃ§ekmece": [41.0002, 28.7801], "Maltepe": [40.9241, 29.1311],
    "Pendik": [40.8801, 29.2501], "Sancaktepe": [40.9905, 29.2201], "SarÄ±yer": [41.1681, 29.0572],
    "Silivri": [41.0742, 28.2471], "Sultanbeyli": [40.9654, 29.2673], "Sultangazi": [41.1071, 28.8681],
    "Åile": [41.1754, 29.6101], "ÅiÅŸli": [41.0601, 28.9876], "Tuzla": [40.8401, 29.3201],
    "Ãœmraniye": [41.0256, 29.0963], "ÃœskÃ¼dar": [41.0261, 29.0152], "Zeytinburnu": [40.9904, 28.9001],
    "Ã‡atalca": [41.1432, 28.4593], "BahÃ§elievler": [41.0001, 28.8601], "GaziosmanpaÅŸa": [41.0581, 28.9124],
    "Fatih": [41.0102, 28.9403], "GÃ¼ngÃ¶ren": [41.0253, 28.8651]
}

# --- TÃœM Ã–ZELLÄ°K KELÄ°MELERÄ° (KullanÄ±m kolaylÄ±ÄŸÄ± iÃ§in kÄ±saltÄ±ldÄ±) ---
ALL_KEYWORDS = ['bÃ¶lge', 'mahalle', 'semt', 'ilan tarihi', 'kat konumu', 'kat sayÄ±sÄ±', 'mobilyalÄ±', 'kullanÄ±m durumu', 'kiralamaya uygun', 
    'kimden', 'takas', 'batÄ± cephe', 'doÄŸu cephe', 'gÃ¼ney cephe', 'kuzey cephe', 'adsl', 'ahÅŸap doÄŸrama', 'akÄ±llÄ± ev', 
    'alarmÄ± (hÄ±rsÄ±z)', 'alarm (yangÄ±n)', 'alaturka tuvalet', 'alÃ¼minyum doÄŸrama', 'amerikan kapÄ±', 'amerikan mutfak', 
    'ankastre', 'asansÃ¶r', 'barbekÃ¼', 'ev aletleri', 'boyalÄ±', 'bulaÅŸÄ±k makinesi', 'buzdolabÄ±', 'duvar kaÄŸÄ±dÄ±', 'duÅŸ', 
    'ebeveyn banyosu', 'fiber internet', 'giyinme odasÄ±', 'dolap', 'gÃ¶rÃ¼ntÃ¼lÃ¼ interkom', 'hilton banyosu', 'interkom sistemi', 
    'yalÄ±tÄ±mlÄ± cam', 'jakuzi', 'alÃ§Ä±pan', 'bodrum', 'klima', 'kÃ¼vet', 'laminat parke', 'marley mobilya', 'ankastre mutfak', 
    'laminat mutfak', 'doÄŸalgazlÄ± mutfak', 'pvc doÄŸrama', 'jaluzi', 'parke zemin', 'seramik zemin', 'set Ã¼stÃ¼ ocak', 
    'spot aydÄ±nlatma', 'teras', 'termosifon', 'vestiyer', 'wi-fi', 'yÃ¼z tanÄ±ma ve parmak iÌ‡zi', 'Ã§amaÅŸÄ±r makinesi', 
    'Ã§amaÅŸÄ±rhane', 'Ã§elik kapÄ±', 'su Ä±sÄ±tÄ±cÄ±', 'ÅŸÃ¶mine', 'buhar odasÄ±', 'gÃ¼venlik banyosu', 'gÃ¼Ã§lendirici', 'Ä±sÄ± yalÄ±tÄ±mÄ±', 
    'jeneratÃ¶r', 'kablo tv', 'kapalÄ± garaj', 'kapÄ±cÄ±', 'kreÅŸ', 'Ã¶zel havuzlu', 'otopark', 'oyun alanÄ±', 'sauna', 
    'ses yalÄ±tÄ±mÄ±', 'dÄ±ÅŸ cephe kaplamasÄ±', 'spor alanÄ±', 'su deposu', 'tenis kortu', 'uydu', 'yangÄ±n merdiveni', 
    'aÃ§Ä±k yÃ¼zme havuzu', 'kapalÄ± yÃ¼zme havuzu', 'geniÅŸ koridor', 'giriÅŸ / rampa', 'merdivenler', 'oda kapÄ±sÄ±', 'priz / elektrik anahtarÄ±', 
    'kapÄ± kolu / korkuluk', 'tuvalet', 'yÃ¼zme havuzu', 'alÄ±ÅŸveriÅŸ merkezi', 'belediye', 'cami', 'cemevi', 'sahile yakÄ±n', 
    'eczane', 'eÄŸlence merkezi', 'fuar', 'hastane', 'sinagog', 'kilise', 'lise', 'market', 'park', 'polis karakolu', 
    'saÄŸlÄ±k kliniÄŸi', 'ilÃ§e marketi', 'spor salonu', 'Ã¼niversite', 'ilkokul-ortaokul', 'itfaiye', 'ÅŸehir merkezi', 
    'otoyol', 'avrasya tÃ¼neli', 'boÄŸaz kÃ¶prÃ¼leri', 'cadde', 'deniz otobÃ¼sÃ¼', 'dolu', 'e-5', 'havaalanÄ±', 'marmaray', 
    'metro', 'metrobÃ¼s', 'minibÃ¼s', 'otobÃ¼s duraÄŸÄ±', 'sahil', 'tem', 'teleferik', 'tramvay', 'tren iÌ‡stasyonu', 
    'troleybÃ¼s', 'iskele', 'boÄŸaz denizi', 'doÄŸa', 'gÃ¶l', 'havuz', 'park ve yeÅŸil alan', 'ÅŸehir', 'asma kat', 
    'ara kat dubleks', 'bahÃ§e dubleks', 'bahÃ§e katÄ±', 'bahÃ§e Ã¼st kat', 'garaj / dÃ¼kkan', 'Ã¼st giriÅŸ katÄ±', 'kat dubleks', 
    'Ã¶zel giriÅŸ', 'rerse dubleks', 'tripleks', 'zemin kat', 'Ã§atÄ± dubleks', 'teslim alma zamanÄ±'
]

# --- VERÄ° YÃœKLEME (Temel ve Teknik Temizlik) ---
@st.cache_data
def load_data(file_path):
    try:
        if isinstance(file_path, str):
            try: df = pd.read_csv(file_path, sep=None, encoding='utf-8', engine='python')
            except: df = pd.read_csv(file_path, sep=',', encoding='utf-8')
        else:
            df = pd.read_csv(file_path, sep=None, encoding='utf-8', engine='python')

        df.columns = df.columns.str.strip()
        
        col_map = {}
        all_cols = list(df.columns)
        
        def find_col(keywords, target_name, mandatory=False):
            for col in all_cols:
                if any(k.lower() in col.lower() for k in keywords):
                    col_map[col] = target_name
                    return col
            if mandatory: return None
            return 'not_found'

        # Kritik SÃ¼tunlarÄ±n EÅŸlenmesi
        if not find_col(['district', 'ilÃ§e'], 'District', mandatory=True): return None, "Kritik 'Ä°lÃ§e' sÃ¼tunu eksik.", None
        if not find_col(['price', 'fiyat'], 'Price', mandatory=True): return None, "Kritik 'Fiyat' sÃ¼tunu eksik.", None
        if not (find_col(['mÂ² (net)', 'net m2', 'net'], 'mÂ²') or find_col(['mÂ² (brÃ¼t)', 'gross'], 'mÂ²')): return None, "Kritik 'mÂ²' sÃ¼tunu eksik.", None
        
        find_col(['neighborhood', 'mahalle'], 'Neighborhood')
        find_col(['oda', 'room'], 'Oda_Text')
        find_col(['bina yaÅŸÄ±', 'building age', 'age'], 'Bina_Yasi')
        find_col(['banyo sayÄ±sÄ±', 'number of bathrooms', 'bath'], 'Banyo_Sayisi')
        find_col(['Ä±sÄ±tma', 'heating'], 'Isitma')
        find_col(['kat konumu'], 'Kat_Konumu') # MantÄ±ksal kontrol
        find_col(['kat sayÄ±sÄ±'], 'Kat_Sayisi') # MantÄ±ksal kontrol
        find_col(['teras'], 'Teras') # MantÄ±ksal kontrol
        
        # CoÄŸrafi MantÄ±k KontrolÃ¼ iÃ§in SÃ¼tunlar
        find_col(['sahile yakÄ±n', 'deniz kenarÄ±'], 'Sahile_Yakin')
        find_col(['gÃ¶l'], 'GÃ¶l')
        find_col(['yÃ¼zme havuzu', 'havuz', 'Ã¶zel havuzlu'], 'Yuzme_Havuzu')
        find_col(['boÄŸaz denizi', 'boÄŸaz kÃ¶prÃ¼leri', 'deniz'], 'Bogaz_Deniz')
        find_col(['doÄŸa', 'park ve yeÅŸil alan'], 'Doga_Yesil_Alan') 

        df = df.rename(columns=col_map)

        def clean_num(val):
            if isinstance(val, str): 
                val = re.sub(r'[^\d.]', '', val.replace('TL', '').replace('.', '').replace(',', '.'))
            try: return float(val)
            except: return np.nan

        df['Price'] = df['Price'].apply(clean_num)
        df['mÂ²'] = df['mÂ²'].apply(clean_num)
        
        if 'Banyo_Sayisi' in df.columns:
            df['Banyo_Sayisi'] = df['Banyo_Sayisi'].apply(lambda x: clean_num(x) if pd.notna(x) else 0)
        
        if 'Bina_Yasi' in df.columns:
            df['Bina_Yasi'] = df['Bina_Yasi'].apply(lambda x: clean_num(x) if pd.notna(x) else 0)
        if 'Kat_Sayisi' in df.columns:
            df['Kat_Sayisi'] = df['Kat_Sayisi'].apply(lambda x: clean_num(x) if pd.notna(x) else 0)

        initial_count = len(df)
        df_cleaned = df.copy()
        
        df_cleaned = df_cleaned[df_cleaned.get('Bina_Yasi', 0).apply(lambda x: x <= 125 and x >= 0)] 
        
        df_cleaned = df_cleaned.dropna(subset=['Price', 'mÂ²'])
        df_cleaned = df_cleaned[(df_cleaned['Price'] > 0) & (df_cleaned['mÂ²'] > 0) & (df_cleaned['mÂ²'] < 1000)]
        
        removed_count = initial_count - len(df_cleaned)
        st.session_state['cleaning_report'] = {"initial": initial_count, "removed": removed_count, "final": len(df_cleaned)}

        df = df_cleaned 

        df['District'] = df['District'].str.title().str.strip()
        
        def parse_room(val):
            try:
                nums = re.findall(r'\d+', str(val))
                return int(nums[0]) if nums else 2
            except: return 2
        
        if 'Oda_Text' in df.columns: df['Oda_Sayisi'] = df['Oda_Text'].apply(parse_room)
        else: df['Oda_Sayisi'] = 2

        # Feature kolonlarÄ±nÄ± boolean'a Ã§evir
        feature_cols = []
        for col_name in df.columns: 
            if col_name not in ['District', 'Price', 'mÂ²', 'Oda_Text', 'Bina_Yasi', 'Banyo_Sayisi', 'Isitma', 'Oda_Sayisi', 'Neighborhood', 'Kat_Konumu', 'Kat_Sayisi']:
                if df[col_name].dtype in [np.int64, np.float64, bool] and df[col_name].nunique() <= 2 and df[col_name].max() <= 1:
                    df[col_name] = df[col_name].fillna(0).apply(lambda x: 1 if x > 0 else 0)
                    if col_name not in feature_cols:
                        feature_cols.append(col_name)
                        
        return df, None, feature_cols

    except Exception as e: return None, str(e), None

# --- Yapay Zeka AsistanÄ± Fonksiyonu (Global Kapsamda) ---
def smart_data_assistant_multilang(df, query, lang_code):
    query = query.lower()
    k = TRANS.get(lang_code, TRANS["TR"])
    
    if any(x in query for x in ["toplam", "total", "Ğ²ÑĞµĞ³Ğ¾", "Ù…Ø¬Ù…ÙˆØ¹", "kaÃ§ ilan", "how many"]):
        return k["ai_total_resp"].format(count=len(df))
    
    if any(x in query for x in ["pahalÄ±", "expensive", "Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¹", "Ø£ØºÙ„Ù‰"]):
        row = df.sort_values(by='Price', ascending=False).iloc[0]
        return f"**{row['District']}** ({row['mÂ²']} mÂ²): {row['Price']:,.0f} TL"
    
    if any(x in query for x in ["ucuz", "cheap", "Ğ´ĞµÑˆĞµĞ²Ñ‹Ğ¹", "Ø£Ø±Ø®Øµ"]):
        temp_df = df[df['Price'] > 10000].sort_values(by='Price', ascending=True)
        if not temp_df.empty:
            row = temp_df.iloc[0]
            return f"**{row['District']}** ({row['mÂ²']} mÂ²): {row['Price']:,.0f} TL"
        else:
            return "Ã‡ok ucuz mantÄ±klÄ± ilan bulunamadÄ±."
    
    if any(x in query for x in ["ortalama mÂ²", "average m2 price", "ÑÑ€ĞµĞ´Ğ½ÑÑ Ñ†ĞµĞ½Ğ° Ğ¼Â²", "Ù…ØªÙˆØ³Ø· Ø³Ø¹Ø± Ø§Ù„Ù…ØªØ±"]):
        avg_price_m2 = (df['Price'] / df['mÂ²']).mean()
        return f"Ortalama mÂ² fiyatÄ±: **{avg_price_m2:,.0f} TL**"

    if any(x in query for x in ["ortalama yaÅŸ", "average age", "ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚", "Ù…ØªÙˆØ³Ø· Ø¹Ù…Ø± Ø§Ù„Ù…Ø¨Ù†Ù‰"]) and 'Bina_Yasi' in df.columns:
        avg_age = df['Bina_Yasi'].mean()
        return f"Ortalama Bina YaÅŸÄ±: **{avg_age:,.1f}**"
    
    districts = df['District'].unique()
    for d in districts:
        if d.lower() in query:
            dist_df = df[df['District'] == d]
            avg_price = dist_df['Price'].mean()
            count = len(dist_df)
            return f"ğŸ“ **{d}** Analizi:\n- Ä°lan SayÄ±sÄ±: {count}\n- Ort. Fiyat: **{avg_price:,.0f} TL**"

    return k["ai_unknown"]

# --- MANTIK KONTROL FONKSÄ°YONU ---
def check_logical_inconsistencies(df):
    """
    MantÄ±ksal olarak imkÃ¢nsÄ±z / absÃ¼rt ilanlarÄ± otomatik tespit eder.
    True dÃ¶nen satÄ±rlar HATALI kabul edilir ve analizden silinir.
    """

    mask = pd.Series(False, index=df.index)

    # --- DENÄ°Z / GÃ–L / HAVUZ / DOÄA Ã‡AKIÅMASI ---
    cols = []
    for c in ['Bogaz_Deniz', 'GÃ¶l', 'Yuzme_Havuzu', 'Doga_Yesil_Alan']:
        if c in df.columns:
            cols.append(c)

    if len(cols) >= 2:
        mask |= (df[cols].sum(axis=1) >= 3)

    # --- DENÄ°Z + GÃ–L (KESÄ°N HATA) ---
    if 'Bogaz_Deniz' in df.columns and 'GÃ¶l' in df.columns:
        mask |= (df['Bogaz_Deniz'] == 1) & (df['GÃ¶l'] == 1)

    # --- TERAS + ZEMÄ°N / BAHÃ‡E KAT ---
    if 'Kat_Konumu' in df.columns and 'Teras' in df.columns:
        kat = df['Kat_Konumu'].astype(str).str.lower()
        mask |= (df['Teras'] == 1) & (kat.str.contains('zemin|bahÃ§e|giriÅŸ', na=False))

    # --- YÃœKSEK BÄ°NA + ZEMÄ°N KAT ---
    if 'Kat_Sayisi' in df.columns and 'Kat_Konumu' in df.columns:
        kat = df['Kat_Konumu'].astype(str).str.lower()
        mask |= (df['Kat_Sayisi'] >= 8) & (kat.str.contains('zemin|bahÃ§e', na=False))

    # --- AÅIRI mÂ² (DAÄ°RE Ä°Ã‡Ä°N) ---
    mask |= (df['mÂ²'] < 15) | (df['mÂ²'] > 800)

    # --- AÅIRI ESKÄ° BÄ°NA ---
    if 'Bina_Yasi' in df.columns:
        mask |= (df['Bina_Yasi'] > 150)

    # --- FÄ°YAT / mÂ² ABSÃœRTLÃœÄÃœ ---
    birim_fiyat = df['Price'] / df['mÂ²']
    mask |= (birim_fiyat < 1000) | (birim_fiyat > 500_000)

    return mask


def train_model_and_compare(df_raw_for_train, feature_cols, lang_code):
    
    df = df_raw_for_train.copy()
    
    # --- 1. ABSÃœRT MANTIK HATALARINI SÄ°LME ---
    logical_errors = check_logical_inconsistencies(df)
    
    df_absurd_removed = df[~logical_errors].copy()
    
    removed_absurd = len(df) - len(df_absurd_removed)
    df = df_absurd_removed 
    
    initial_for_ai_check = len(df) 
    
    # --- 2. AI MODELÄ°NÄ° EÄÄ°TME VE TAHMÄ°N ---
    
    df_train = df.copy() 
    
    # UÃ§ DeÄŸerleri Atma (Fiyat/mÂ² outlier)
    df_train['Birim_Fiyat'] = df_train['Price'] / df['mÂ²'] 
    Q1 = df_train['Birim_Fiyat'].quantile(0.05) 
    Q3 = df_train['Birim_Fiyat'].quantile(0.95)
    df_train = df_train[(df_train['Birim_Fiyat'] >= Q1) & (df_train['Birim_Fiyat'] <= Q3)]

    # Target Encoding (Konum Skoru)
    if 'Neighborhood' in df.columns:
        neigh_map = df_train.groupby('Neighborhood').apply(lambda x: (x['Price']/x['mÂ²']).median()).to_dict()
        df['Konum_Skoru'] = df['Neighborhood'].map(neigh_map).fillna(df['Price'].median()/df['mÂ²'].median())
        df_train['Konum_Skoru'] = df['Neighborhood'].map(neigh_map)
    else:
        dist_map = df_train.groupby('District').apply(lambda x: (x['Price']/x['mÂ²']).median()).to_dict()
        df['Konum_Skoru'] = df['District'].map(dist_map)
        df_train['Konum_Skoru'] = df['District'].map(dist_map)

    features = ['mÂ²', 'Oda_Sayisi', 'Konum_Skoru'] + feature_cols
    if 'Bina_Yasi' in df.columns: features.append('Bina_Yasi')
    if 'Banyo_Sayisi' in df.columns: features.append('Banyo_Sayisi')

    X_train_data = df_train[features].fillna(0)
    y_train_data = np.log1p(df_train['Price']) 
    
    X_train, X_test, y_train, y_test = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=42)
    
    model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    r2 = r2_score(np.expm1(y_test), np.expm1(y_pred))

    # TÃ¼m Dataya Uygula
    X_full = df[features].fillna(0)
    df['AI_Tahmin'] = np.expm1(model.predict(X_full))
    
    # 2020 Referans
    def get_ref_price(district, m2):
        for key, val in REF_PRICES_2020.items():
            if key.lower() in district.lower(): return val * m2
        return None

    df['Ref_2020_Deger'] = df.apply(lambda row: get_ref_price(row['District'], row['mÂ²']), axis=1)
    
    # Durum Belirleme (Sadece Fiyat SapmasÄ±nÄ± Kontrol Eder)
    def determine_status(row):
        target = row['Ref_2020_Deger'] if pd.notnull(row['Ref_2020_Deger']) else row['AI_Tahmin']
        if target == 0 or pd.isna(target): return "N/A"
        
        diff = ((row['Price'] - target) / target) * 100
        
        current_trans = TRANS[lang_code]
        if diff < -40: return current_trans["status_err_low"] # Ã‡OK Ucuz Fiyat HatasÄ± (Silinecek)
        if -40 <= diff < -15: return current_trans["status_opp"]
        if -15 <= diff <= 25: return current_trans["status_ok"]
        return current_trans["status_err_high"] # Ã‡OK PahalÄ± Fiyat HatasÄ± (Silinecek)

    df['Durum'] = df.apply(determine_status, axis=1)
    df['Sapma_Genel_%'] = df.apply(lambda row: ((row['Price'] - (row['Ref_2020_Deger'] if pd.notnull(row['Ref_2020_Deger']) else row['AI_Tahmin'])) / (row['Ref_2020_Deger'] if pd.notnull(row['Ref_2020_Deger']) else row['AI_Tahmin'])) * 100, axis=1)

    # --- 3. AI Fiyat HatalarÄ±nÄ± SÄ°LME (Nihai Temizlik) ---
    valid_statuses = [TRANS[lang_code]["status_opp"], TRANS[lang_code]["status_ok"]]
    
    df_final = df[df['Durum'].isin(valid_statuses)].copy() 
    
    removed_ai_outliers = initial_for_ai_check - len(df_final)

    # TEMÄ°ZLÄ°K RAPORUNU GÃœNCELLE
    initial_tech_removed = st.session_state['cleaning_report']['removed']
    total_removed = initial_tech_removed + removed_absurd + removed_ai_outliers
    
    st.session_state['cleaning_report']['removed'] = total_removed
    st.session_state['cleaning_report']['final'] = len(df_final)
    
    return df_final, r2

# --- ARAYÃœZ BAÅLANGIÃ‡ ---
selected_lang_code = st.session_state.get('selected_lang', 'TR')
T = TRANS[selected_lang_code]

# CSS / RTL AyarlarÄ± (KÄ±saltÄ±ldÄ±)
direction = "rtl" if selected_lang_code == "AR" else "ltr"
align = "right" if selected_lang_code == "AR" else "left"

st.markdown(f"""
<style>
    .stApp {{ direction: {direction}; }}
    h1, h2, h3 {{ text-align: {align}; }}
    .user-msg, .ai-msg {{ text-align: {align}; direction: {direction}; }}
</style>
""", unsafe_allow_html=True)


with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/1040/1040993.png", width=80)
    st.title(T["title"])
    
    new_lang = st.selectbox("Language / Dil / Ğ¯Ğ·Ñ‹Ğº / Ø§Ù„Ù„ØºØ©", ["TR", "EN", "RU", "AR"], index=["TR", "EN", "RU", "AR"].index(selected_lang_code))
    if new_lang != selected_lang_code:
        st.session_state['selected_lang'] = new_lang
        st.rerun()
    
    uploaded_file = st.file_uploader(T["upload_label"], type=['csv'])
    if not uploaded_file and os.path.exists("veri.csv"): uploaded_file = "veri.csv"

    df_raw = None
    err = None
    feats = None
    if uploaded_file:
        df_raw, err, feats = load_data(uploaded_file)
    
    if err:
        st.error(err)
        st.stop()
        
    if df_raw is None:
        st.info(T["err_no_file"])
        st.stop()
        
    st.success(f"âœ… {len(df_raw):,} {T['upload_success']}")
    
    if st.button(T["btn_analyze"], type="primary"):
        with st.spinner(T["loading"]):
            df_res, score = train_model_and_compare(df_raw, feats, new_lang)
            st.session_state['data'] = df_res
            st.session_state['score'] = score
            st.session_state['feats'] = feats
    


# --- ANA EKRAN ---
if 'data' in st.session_state:
    df = st.session_state['data']
    r2 = st.session_state['score']
    
    # TÃ¼m SÃ¼tunlarÄ± Yakalama
    IGNORE_COLS = ['Oda_Text', 'Neighborhood', 'Konum_Skoru', 'Birim_Fiyat', 'Kat_Konumu_Str'] 
    ALL_DISPLAY_COLS = [col for col in df.columns if col not in IGNORE_COLS]
    
    # Ã‡eviri SÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur
    col_config = {}
    for col in ALL_DISPLAY_COLS:
        if col == 'Price': col_config[col] = st.column_config.NumberColumn(T["col_price"], format="%d TL")
        elif col == 'AI_Tahmin': col_config[col] = st.column_config.NumberColumn(T["col_ai"], format="%d TL")
        elif col == 'Ref_2020_Deger': col_config[col] = st.column_config.NumberColumn(T["col_ref"], format="%d TL")
        elif col == 'Sapma_Genel_%': col_config[col] = st.column_config.NumberColumn(T["col_diff"], format="%.1f%%")
        elif col == 'Oda_Sayisi': col_config[col] = T["col_rooms"]
        elif col == 'Banyo_Sayisi': col_config[col] = T["col_bath"]
        elif col == 'Bina_Yasi': col_config[col] = T["col_age"]
        elif col == 'Isitma': col_config[col] = T["col_heating"]
        elif df[col].dtype in [np.int64, np.float64, bool] and col not in ['Price', 'mÂ²']:
             df[col] = df[col].apply(lambda x: T["val_none"] if (x == 0 or x is False) else x)
             if col not in col_config:
                 col_config[col] = col
        elif col not in col_config:
             col_config[col] = col

    # METRÄ°K KARTLARI
    c1, c2, c3, c4 = st.columns(4)
    opp_count = len(df[df['Durum'] == T["status_opp"]])
    final_count = st.session_state['cleaning_report']['final']
    initial_count = st.session_state['cleaning_report']['initial']
    removed_count = st.session_state['cleaning_report']['removed']
    
    r2_color = "#00e676" if r2 > 0.85 else ("#ffab00" if r2 > 0.75 else "#ff1744")
    
    with c1: st.markdown(f'<div class="metric-container" style="border-color:{r2_color}"><div class="metric-value" style="color:{r2_color}">%{r2*100:.1f}</div><div class="metric-label">{T["metric_acc"]}</div></div>', unsafe_allow_html=True)
    with c2: st.markdown(f'<div class="metric-container"><div class="metric-value">{final_count:,}</div><div class="metric-label">{T["metric_total"]}</div></div>', unsafe_allow_html=True)
    with c3: st.markdown(f'<div class="metric-container" style="border-color:#00e676"><div class="metric-value" style="color:#00e676">{opp_count:,}</div><div class="metric-label">{T["metric_opp"]}</div></div>', unsafe_allow_html=True)
    with c4: st.markdown(f'<div class="metric-container" style="border-color:#ff1744"><div class="metric-value" style="color:#ff1744">{removed_count:,}</div><div class="metric-label">{T["metric_err"]}</div></div>', unsafe_allow_html=True)

    # Temizlik Raporu
    st.info(f"**{T['cleaning_report'].format(initial=initial_count, removed=removed_count, final=final_count)}**")
    
    st.write("")
    
    # SEKMELER
    tabs = st.tabs([T["tab_detail"], T["tab_opp"], T["tab_comp"], T["tab_map"], T["tab_ai"]])

    # 1. DETAYLI ANALÄ°Z (TÃ¼m SÃ¼tunlar KaydÄ±rÄ±labilir)
    with tabs[0]:
        st.markdown(f"### ğŸ“Š {T['tab_detail']}")
        
        st.dataframe(df[ALL_DISPLAY_COLS], column_config=col_config, use_container_width=True, height=600)

    # 2. FIRSATLAR
    with tabs[1]:
        st.success(T["box_opp_title"])
        opps = df[df['Durum'] == T["status_opp"]].sort_values(by='Sapma_Genel_%', ascending=False)
        st.dataframe(opps[ALL_DISPLAY_COLS], column_config=col_config, use_container_width=True)
    
    # 3. OPTIMAL SEÃ‡Ä°M (KARÅILAÅTIRMA)
    with tabs[2]:
        st.markdown(f"### ğŸ¯ {T['tab_comp']}")
        
        clean_df = df.copy() # Zaten temizlenmiÅŸ
        
        comparison_options = {
            T["comp_rooms"]: "Oda SayÄ±sÄ±na GÃ¶re", 
            T["comp_price"]: "Fiyata GÃ¶re", 
            T["comp_m2"]: "mÂ²'ye GÃ¶re"
        }
        selected_option_key = st.selectbox(T["comp_select_title"], list(comparison_options.keys()))
        comparison_type = comparison_options[selected_option_key]

        if comparison_type == "Oda SayÄ±sÄ±na GÃ¶re":
            kriter_name = selected_option_key
            st.markdown(f"#### {T['comp_list_title'].format(kriter=kriter_name)}")
            
            best_by_room = clean_df.loc[clean_df.groupby('Oda_Sayisi')['Sapma_Genel_%'].idxmax()].sort_values(by='Sapma_Genel_%', ascending=False)
            
            st.dataframe(best_by_room[ALL_DISPLAY_COLS], column_config=col_config, use_container_width=True)
            
        elif comparison_type == "Fiyata GÃ¶re":
            kriter_name = selected_option_key
            st.markdown(f"#### {T['comp_list_title'].format(kriter=kriter_name)}")
            
            best_by_roi = clean_df.sort_values(by='Sapma_Genel_%', ascending=False).head(10)
            st.dataframe(best_by_roi[ALL_DISPLAY_COLS], column_config=col_config, use_container_width=True)

        elif comparison_type == "mÂ²'ye GÃ¶re":
            kriter_name = selected_option_key
            st.markdown(f"#### {T['comp_list_title'].format(kriter=kriter_name)}")
            
            best_by_m2_value = clean_df.sort_values(by='Sapma_Genel_%', ascending=False).head(10)
            st.dataframe(best_by_m2_value[ALL_DISPLAY_COLS], column_config=col_config, use_container_width=True)


    # 4. HARÄ°TA (TÃ¼m BÃ¶lgeler GÃ¶rÃ¼nÃ¼r)
    with tabs[3]:
        st.markdown(f"### {T['map_title']}")
        try:
            m = folium.Map(location=[41.0082, 28.9784], zoom_start=9, tiles="CartoDB dark_matter")
            dist_summary = df.groupby('District').agg({'Price': 'mean', 'mÂ²': 'count', 'Sapma_Genel_%': 'mean'}).reset_index()
            
            for idx, row in dist_summary.iterrows():
                coords = ISTANBUL_COORDS.get(row['District'])
                
                if coords:
                    color = "#00e676" if row['Sapma_Genel_%'] < -5 else ("#ff1744" if row['Sapma_Genel_%'] > 10 else "#29b6f6")
                    
                    folium.CircleMarker(
                        location=coords, 
                        radius=5 + (row['mÂ²'] / dist_summary['mÂ²'].max() * 20),
                        popup=f"<b>{row['District']}</b><br>{T['col_price']}: {row['Price']:,.0f} TL",
                        color=color, 
                        fill=True, 
                        fill_color=color, 
                        fill_opacity=0.6
                    ).add_to(m)

            st_folium(m, width="100%", height=500)
        except Exception as e: st.error(f"Harita hatasÄ±: {e}")

    # 5. AI ASÄ°STAN (KÄ±lavuz Ä°Ã§inde)
    with tabs[4]:
        st.markdown(f"### {T['tab_ai']}")
        chat_col, info_col = st.columns([3, 1])
        
        with info_col:
            st.markdown(f"#### {T['ai_guide_title']}")
            st.info(T['ai_guide_content']) # KÄ±lavuz buraya taÅŸÄ±ndÄ±.
        
        with chat_col:
            chat_cont = st.container(height=400, border=True)
            if "messages" not in st.session_state: st.session_state.messages = []
            
            for msg in st.session_state.messages:
                cls = "user-msg" if msg["role"] == "user" else "ai-msg"
                chat_cont.markdown(f"<div class='{cls}'>{msg['content']}</div>", unsafe_allow_html=True)
                
            if prompt := st.chat_input(T["chat_placeholder"]):
                st.session_state.messages.append({"role": "user", "content": prompt})
                chat_cont.markdown(f"<div class='user-msg'>{prompt}</div>", unsafe_allow_html=True)
                
                response = smart_data_assistant_multilang(df, prompt, selected_lang_code)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                chat_cont.markdown(f"<div class='ai-msg'>{response}</div>", unsafe_allow_html=True)
